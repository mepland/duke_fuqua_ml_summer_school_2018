{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Assignment: Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Matthew Epland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "Build a 2-layer CNN for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image -> Convolution (32 5x5 filters) -> nonlinearity (ReLU) ->  (2x2 max pool) -> Convolution (64 5x5 filters) -> nonlinearity (ReLU) -> (2x2 max pool) -> fully connected (256 hidden units) -> nonlinearity (ReLU) -> fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Some tips:\n",
    "- The CNN model might take a while to train. Depending on your machine, you might expect this to take up to half an hour. If you see your validation performance start to plateau, you can kill the training.\n",
    "\n",
    "- Since CNNs a more complex than the logistic regression and MLP models you've worked with before, so you may find it helpful to use a more advanced optimizer. You're model will train faster if you use [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) instead of `tf.train.GradientDescentOptimizer`. A learning rate of 1e-4 is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tqdm import trange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-150ee052585e>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/mepland/.virtualenvs/fuquaml/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/mepland/.virtualenvs/fuquaml/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/mepland/.virtualenvs/fuquaml/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/mepland/.virtualenvs/fuquaml/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/mepland/.virtualenvs/fuquaml/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image -> Convolution (32 5x5 filters) -> nonlinearity (ReLU) ->  (2x2 max pool) -> Convolution (64 5x5 filters) -> nonlinearity (ReLU) -> (2x2 max pool) -> fully connected (256 hidden units) -> nonlinearity (ReLU) -> fully connected (10 hidden units) -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# input layers\n",
    "x = tf.placeholder(tf.float32, [None,28*28])\n",
    "x_2D = tf.reshape(x, [-1, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "# Convolution (32 5x5 filters) -> nonlinearity (ReLU)\n",
    "W_1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1))\n",
    "b_1 = tf.Variable(tf.zeros([32]))\n",
    "conv_1 = tf.nn.relu(tf.nn.conv2d(x_2D, W_1, strides=[1,1,1,1], padding=\"SAME\") + b_1)\n",
    "\n",
    "# (2x2 max pool)\n",
    "max_pool_1 = tf.nn.max_pool(conv_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "# Convolution (64 5x5 filters) -> nonlinearity (ReLU)\n",
    "W_2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1))\n",
    "b_2 = tf.Variable(tf.zeros([64]))\n",
    "conv_2 = tf.nn.relu(tf.nn.conv2d(max_pool_1, W_2, strides=[1,1,1,1], padding=\"SAME\") + b_2)\n",
    "\n",
    "# (2x2 max pool)\n",
    "max_pool_2 = tf.nn.max_pool(conv_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "# flatten for fully connected layer\n",
    "flat = tf.reshape(max_pool_2, [-1, 7*7*64])\n",
    "\n",
    "# fully connected (256 hidden units) -> nonlinearity (ReLU)\n",
    "W_3 = tf.Variable(tf.truncated_normal([7*7*64, 256], stddev=0.1))\n",
    "b_3 = tf.Variable(tf.zeros([256]))\n",
    "fc_1 = tf.nn.relu(tf.matmul(flat, W_3) + b_3)\n",
    "\n",
    "# fully connected (10 hidden units)\n",
    "W_4 = tf.Variable(tf.truncated_normal([256, 10], stddev=0.1))\n",
    "b_4 = tf.Variable(tf.zeros([10]))\n",
    "fc_2 = tf.matmul(fc_1, W_4) + b_4\n",
    "\n",
    "# softmax and cross entropy (tf.nn.softmax_cross_entropy_with_logits is deprecated)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=fc_2, labels=y)\n",
    "avg_loss = tf.reduce_mean(loss)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [32:23<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# sess.run(tf.initialize_all_variables()) # deprecated\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train in multiple (10000) \"epochs\"\n",
    "# really ~18 epochs in the D1 MLP notation with 550 mini batches of 100 images\n",
    "# for simplicity just do both loops in one for loop though\n",
    "for _ in trange(10000):\n",
    "    # sweep through all the training data in mini batchs of 100 images\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test avg loss = 0.03\n",
      "test accuracy = 99.11%\n"
     ]
    }
   ],
   "source": [
    "print('test avg loss = {0:.2f}'.format(sess.run(avg_loss, feed_dict={x:mnist.test.images, y:mnist.test.labels})))\n",
    "\n",
    "y_test_pred = sess.run(fc_2, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "ncorrect = np.sum(np.argmax(y_test_pred, axis=1) == np.argmax(mnist.test.labels, axis=1))\n",
    "print('test accuracy = {0:.2f}%'.format(100*(float(ncorrect) / float(mnist.test.labels.shape[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer\n",
    "\n",
    "1\\. How does the CNN compare in accuracy with yesterday's logistic regression and MLP models? How about training time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesterday's MLP took 01:53 for 50 epochs and had a test accuracy of 97.22%  \n",
    "\n",
    "Today's CNN took 32:23 for 10k \"epochs\" and had a test accuracy of 99.11%  \n",
    "That is a slight improvment at the cost of extra training, but in practice we could have just started with a different network pretrained on MNIST by someone else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. How many trainable parameters are there in the CNN you built for this assignment?\n",
    "\n",
    "*Note: By trainable parameters, I mean individual scalars. For example, a weight matrix that is 10x5 has 50.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2570"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5*5*1*32)+32\n",
    "+(5*5*32*64)+64\n",
    "+(7*7*64*256)+256\n",
    "+(256*10)+10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. When would you use a CNN versus a logistic regression model or an MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN: Image classification, very high dimensionality inputs where each variable should be processed in relation to it's neighbors  \n",
    "Logistic regression / MLP: moderate dimensionality inputs, variables are not closely related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
